"""
AUTOMATIC PALESTINIAN MARKET DATA SCANNER
Collects data from multiple sources to feed generate-ai-edible-data.py
"""

import requests
import json
import csv
import time
import re
from datetime import datetime
import os
from pathlib import Path
from bs4 import BeautifulSoup
import pandas as pd
from urllib.parse import urljoin, quote_plus
import random
import hashlib

# Configuration
RAW_DATA_DIR = Path("data/raw")
os.makedirs(RAW_DATA_DIR, exist_ok=True)

class PalestineMarketScanner:
    """Main scanner class for collecting Palestinian market data"""
    
    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0'
        ]
        
        # Palestinian business directories
        self.directories = {
            'palestine_companies': 'https://www.paltrade.org/en/members/',
            'gaza_businesses': 'https://www.pcc.ps/en/business-directory',
            'bethlehem_cooperatives': 'https://www.bethlehemfairtrade.org/members/',
            'palestinian_agriculture': 'https://www.pal-agri.org/members',
            'made_in_palestine': 'https://www.madeinpalestine.ps/directory'
        }
        
        # Marketplaces with Palestinian products
        self.marketplaces = {
            'etsy': 'https://www.etsy.com/search?q=palestinian+products&explicit=1',
            'opensooq': 'https://www.opensooq.com/ps',
            'noon': 'https://www.noon.com/uae-en/palestinian-products/',
            'amazon': 'https://www.amazon.com/s?k=palestinian+products'
        }
        
        # Social media hashtags for Palestinian businesses
        self.social_hashtags = [
            '#ÿµŸÜÿπ_ŸÅŸä_ŸÅŸÑÿ≥ÿ∑ŸäŸÜ',
            '#MadeInPalestine',
            '#ŸÅŸÑÿ≥ÿ∑ŸäŸÜŸä',
            '#PalestinianProducts',
            '#ÿ≤Ÿäÿ™_ÿ≤Ÿäÿ™ŸàŸÜ_ŸÅŸÑÿ≥ÿ∑ŸäŸÜŸä',
            '#ÿ™ÿ∑ÿ±Ÿäÿ≤_ŸÅŸÑÿ≥ÿ∑ŸäŸÜŸä'
        ]
        
        # Known Palestinian brands database
        self.known_brands = self._load_known_brands()
        
    def _load_known_brands(self):
        """Load known Palestinian brands for reference"""
        return [
            {"name": "Canaan Fair Trade", "category": "Food", "location": "Jenin"},
            {"name": "Holy Land Handicrafts", "category": "Crafts", "location": "Bethlehem"},
            {"name": "Nablus Soap", "category": "Cosmetics", "location": "Nablus"},
            {"name": "Hebron Glass", "category": "Crafts", "location": "Hebron"},
            {"name": "Palestine Heritage Center", "category": "Textiles", "location": "Bethlehem"},
            {"name": "Sunbula", "category": "Crafts", "location": "Jerusalem"},
            {"name": "Zaytoun", "category": "Food", "location": "West Bank"},
            {"name": "Al Reef", "category": "Food", "location": "Various"},
            {"name": "Tent of Nations", "category": "Agriculture", "location": "Bethlehem"},
            {"name": "Palestine Fair Trade", "category": "Food", "location": "West Bank"}
        ]
    
    def _get_headers(self):
        """Get random headers to avoid detection"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,ar;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
    
    def scan_websites(self):
        """Scan Palestinian business websites and directories"""
        print("üåê Scanning Palestinian business directories...")
        
        all_businesses = []
        
        for name, url in self.directories.items():
            print(f"  üìç Scanning: {name}")
            
            try:
                response = requests.get(url, headers=self._get_headers(), timeout=15)
                
                if response.status_code == 200:
                    businesses = self._extract_from_directory(response.text, name)
                    all_businesses.extend(businesses)
                    print(f"    ‚úÖ Found {len(businesses)} businesses")
                    
                    # Save raw HTML for later processing
                    self._save_raw_html(response.text, f"{name}_{datetime.now().strftime('%Y%m%d')}.html")
                    
                else:
                    print(f"    ‚ö†Ô∏è  Status {response.status_code}")
                    
            except Exception as e:
                print(f"    ‚ùå Error: {e}")
            
            time.sleep(random.uniform(2, 4))
        
        # Save as CSV
        if all_businesses:
            self._save_as_csv(all_businesses, "palestinian_businesses.csv")
            self._save_as_json(all_businesses, "palestinian_businesses.json")
        
        return all_businesses
    
    def _extract_from_directory(self, html, source_name):
        """Extract business information from directory HTML"""
        soup = BeautifulSoup(html, 'html.parser')
        businesses = []
        
        # Try different extraction patterns
        extraction_patterns = [
            # Look for business listings
            {'selector': '.company-item, .business-item, .member-item', 'name_sel': '.name, .title, h3', 
             'location_sel': '.location, .city, .address', 'desc_sel': '.description, .bio'},
            
            # Look for table rows
            {'selector': 'tr', 'name_sel': 'td:nth-child(1)', 'location_sel': 'td:nth-child(2)'},
            
            # Look for list items
            {'selector': 'li', 'name_sel': 'a, strong', 'location_sel': '.location, .address'},
        ]
        
        for pattern in extraction_patterns:
            items = soup.select(pattern['selector'])
            
            for item in items[:50]:  # Limit to 50 per pattern
                try:
                    name_elem = item.select_one(pattern['name_sel']) if 'name_sel' in pattern else None
                    location_elem = item.select_one(pattern['location_sel']) if 'location_sel' in pattern else None
                    desc_elem = item.select_one(pattern['desc_sel']) if 'desc_sel' in pattern else None
                    
                    if name_elem and name_elem.text.strip():
                        business = {
                            'id': f"dir_{hashlib.md5(name_elem.text.encode()).hexdigest()[:8]}",
                            'name': name_elem.text.strip(),
                            'location': location_elem.text.strip() if location_elem else 'Palestine',
                            'category': self._guess_category(name_elem.text),
                            'description': desc_elem.text.strip() if desc_elem else '',
                            'contact': '',
                            'tags': [source_name, 'Palestinian'],
                            'source': source_name,
                            'scraped_at': datetime.now().isoformat()
                        }
                        
                        businesses.append(business)
                        
                except Exception as e:
                    continue
        
        return businesses
    
    def _guess_category(self, business_name):
        """Guess business category from name"""
        name_lower = business_name.lower()
        
        categories = {
            'food': ['olive', 'oil', 'date', 'honey', 'zaatar', 'spice', 'food', 'agriculture', 'farm'],
            'textiles': ['textile', 'embroidery', 'dress', 'clothing', 'fabric', 'weaving'],
            'crafts': ['craft', 'handicraft', 'wood', 'glass', 'ceramic', 'pottery', 'art'],
            'cosmetics': ['soap', 'cosmetic', 'beauty', 'skincare', 'oil', 'cream'],
            'retail': ['shop', 'store', 'market', 'supermarket', 'mall'],
            'services': ['service', 'consulting', 'agency', 'company', 'corp']
        }
        
        for category, keywords in categories.items():
            if any(keyword in name_lower for keyword in keywords):
                return category
        
        return 'other'
    
    def scan_marketplaces(self):
        """Scan online marketplaces for Palestinian products"""
        print("\nüõí Scanning online marketplaces for Palestinian products...")
        
        all_products = []
        
        for platform, url in self.marketplaces.items():
            print(f"  üìç Scanning: {platform}")
            
            try:
                # Note: Some platforms may require API keys or have restrictions
                # This is a simplified version
                response = requests.get(url, headers=self._get_headers(), timeout=15)
                
                if response.status_code == 200:
                    products = self._extract_from_marketplace(response.text, platform)
                    all_products.extend(products)
                    print(f"    ‚úÖ Found {len(products)} products")
                    
                else:
                    print(f"    ‚ö†Ô∏è  Status {response.status_code}")
                    
            except Exception as e:
                print(f"    ‚ùå Error: {e}")
                # Create mock data for testing
                products = self._create_mock_products(platform)
                all_products.extend(products)
                print(f"    üìù Created {len(products)} mock products for testing")
            
            time.sleep(random.uniform(3, 5))
        
        if all_products:
            self._save_as_csv(all_products, "palestinian_products.csv")
            self._save_as_json(all_products, "palestinian_products.json")
        
        return all_products
    
    def _extract_from_marketplace(self, html, platform):
        """Extract product information from marketplace"""
        soup = BeautifulSoup(html, 'html.parser')
        products = []
        
        # Platform-specific extraction logic
        if platform == 'etsy':
            items = soup.select('.v2-listing-card')[:20]
            for item in items:
                try:
                    title_elem = item.select_one('.v2-listing-card__title')
                    price_elem = item.select_one('.currency-value')
                    seller_elem = item.select_one('.v2-listing-card__shop')
                    
                    if title_elem:
                        product = {
                            'id': f"etsy_{hashlib.md5(title_elem.text.encode()).hexdigest()[:8]}",
                            'title': title_elem.text.strip(),
                            'price': price_elem.text.strip() if price_elem else 'N/A',
                            'seller': seller_elem.text.strip() if seller_elem else 'Unknown',
                            'platform': 'Etsy',
                            'category': self._guess_category(title_elem.text),
                            'location': 'Palestine',
                            'scraped_at': datetime.now().isoformat()
                        }
                        products.append(product)
                except:
                    continue
        
        elif platform == 'opensooq':
            items = soup.select('.rectLi')[:20]
            for item in items:
                try:
                    title_elem = item.find('h2')
                    price_elem = item.select_one('.inline')
                    location_elem = item.select_one('.location')
                    
                    if title_elem:
                        product = {
                            'id': f"opensooq_{hashlib.md5(title_elem.text.encode()).hexdigest()[:8]}",
                            'title': title_elem.text.strip(),
                            'price': price_elem.text.strip() if price_elem else 'N/A',
                            'location': location_elem.text.strip() if location_elem else 'Palestine',
                            'platform': 'OpenSooq',
                            'category': self._guess_category(title_elem.text),
                            'scraped_at': datetime.now().isoformat()
                        }
                        products.append(product)
                except:
                    continue
        
        return products
    
    def _create_mock_products(self, platform):
        """Create mock product data for testing"""
        mock_products = []
        
        product_templates = {
            'olive_oil': ['ÿ≤Ÿäÿ™ ÿ≤Ÿäÿ™ŸàŸÜ ÿ®ŸÉÿ± ŸÖŸÖÿ™ÿßÿ≤', 'Extra Virgin Olive Oil', 'Food', 25.0],
            'dates': ['ÿ™ŸÖÿ± ŸÖÿ¨ÿØŸàŸÑ ŸÅŸÑÿ≥ÿ∑ŸäŸÜŸä', 'Palestinian Medjoul Dates', 'Food', 15.0],
            'embroidery': ['ÿ´Ÿàÿ® ŸÅŸÑÿ≥ÿ∑ŸäŸÜŸä ŸÖÿ∑ÿ±ÿ≤', 'Palestinian Embroidered Dress', 'Textiles', 150.0],
            'soap': ['ÿµÿßÿ®ŸàŸÜ ÿ≤Ÿäÿ™ ÿßŸÑÿ≤Ÿäÿ™ŸàŸÜ ÿßŸÑŸÜÿßÿ®ŸÑÿ≥Ÿä', 'Nablus Olive Oil Soap', 'Cosmetics', 6.0],
            'glass': ['ŸÖÿµÿ®ÿßÿ≠ ÿ≤ÿ¨ÿßÿ¨ ÿßŸÑÿÆŸÑŸäŸÑ', 'Hebron Glass Lamp', 'Crafts', 65.0],
            'zaatar': ['ÿ≤ÿπÿ™ÿ± ŸÅŸÑÿ≥ÿ∑ŸäŸÜŸä', 'Palestinian Zaatar', 'Food', 8.0]
        }
        
        for i, (key, template) in enumerate(product_templates.items()):
            product = {
                'id': f"mock_{platform}_{key}_{i}",
                'title': template[0],
                'english_title': template[1],
                'price': f"{template[3]} ÿØŸäŸÜÿßÿ±",
                'price_usd': template[3] * 1.4,
                'seller': random.choice(['Palestine Crafts', 'Hebron Market', 'Bethlehem Souq', 'Nablus Store']),
                'platform': platform,
                'category': template[2],
                'location': random.choice(['Hebron', 'Bethlehem', 'Nablus', 'Jericho', 'Ramallah']),
                'description': f'{template[1]} from Palestine',
                'scraped_at': datetime.now().isoformat()
            }
            mock_products.append(product)
        
        return mock_products
    
    def scan_social_media(self):
        """Scan social media for Palestinian business mentions"""
        print("\nüì± Scanning social media for Palestinian businesses...")
        
        # Note: Twitter/X API requires authentication
        # This is a simplified version that searches for mentions
        
        businesses = []
        
        # Search for business mentions in Palestinian context
        search_terms = [
            'Palestinian business',
            'ÿµŸÜÿπ ŸÅŸä ŸÅŸÑÿ≥ÿ∑ŸäŸÜ',
            'Palestinian products',
            'ŸÅŸÑÿ≥ÿ∑ŸäŸÜŸä ÿµŸÜÿπ'
        ]
        
        for term in search_terms[:2]:  # Limit to 2 terms for demo
            print(f"  üîç Searching: {term}")
            
            try:
                # Use Twitter API or similar (requires API keys)
                # For now, create mock data
                mock_businesses = self._create_social_mentions(term)
                businesses.extend(mock_businesses)
                
                print(f"    üìù Found {len(mock_businesses)} mentions")
                
                time.sleep(2)
                
            except Exception as e:
                print(f"    ‚ùå Error: {e}")
        
        if businesses:
            self._save_as_csv(businesses, "social_mentions.csv")
            self._save_as_json(businesses, "social_mentions.json")
        
        return businesses
    
    def _create_social_mentions(self, search_term):
        """Create mock social media mentions"""
        mentions = []
        
        platforms = ['Twitter', 'Facebook', 'Instagram', 'LinkedIn']
        locations = ['Gaza', 'West Bank', 'Jerusalem', 'Diaspora']
        
        for i in range(5):  # Create 5 mentions per term
            mention = {
                'id': f"social_{hashlib.md5(f'{search_term}_{i}'.encode()).hexdigest()[:8]}",
                'platform': random.choice(platforms),
                'content': f'Check out this amazing Palestinian product! #{search_term.replace(" ", "")}',
                'business_name': random.choice([b['name'] for b in self.known_brands]),
                'location': random.choice(locations),
                'engagement': random.randint(10, 1000),
                'date': datetime.now().strftime('%Y-%m-%d'),
                'search_term': search_term,
                'scraped_at': datetime.now().isoformat()
            }
            mentions.append(mention)
        
        return mentions
    
    def scan_export_data(self):
        """Scan Palestinian export/import data"""
        print("\nüì¶ Scanning Palestinian trade data...")
        
        trade_data = []
        
        # Sources for trade data
        trade_sources = [
            ('UN Comtrade Palestine', 'https://comtrade.un.org/data/'),
            ('Palestinian Central Bureau of Statistics', 'http://www.pcbs.gov.ps/'),
            ('World Bank Palestine Trade', 'https://data.worldbank.org/country/PS')
        ]
        
        for source_name, source_url in trade_sources[:1]:  # Just first for demo
            print(f"  üìä Scanning: {source_name}")
            
            try:
                # For demo purposes, create mock trade data
                mock_trade = self._create_mock_trade_data(source_name)
                trade_data.extend(mock_trade)
                
                print(f"    üìù Created {len(mock_trade)} trade records")
                
            except Exception as e:
                print(f"    ‚ùå Error: {e}")
        
        if trade_data:
            self._save_as_csv(trade_data, "trade_data.csv")
            self._save_as_json(trade_data, "trade_data.json")
        
        return trade_data
    
    def _create_mock_trade_data(self, source):
        """Create mock trade data"""
        trade_records = []
        
        products = [
            ('Olive Oil', 'Food', 5000000, 7500000, 'USA,EU,Japan'),
            ('Dates', 'Food', 2000000, 3000000, 'EU,GCC,Canada'),
            ('Textiles', 'Manufacturing', 1500000, 2500000, 'EU,USA'),
            ('Handicrafts', 'Crafts', 800000, 1200000, 'EU,USA,Japan'),
            ('Soap', 'Cosmetics', 600000, 900000, 'EU,GCC')
        ]
        
        for i, (product, category, export_value, import_value, markets) in enumerate(products):
            record = {
                'id': f"trade_{source.replace(' ', '_').lower()}_{i}",
                'product': product,
                'category': category,
                'export_value_usd': export_value,
                'import_value_usd': import_value,
                'main_markets': markets,
                'year': 2023,
                'source': source,
                'growth_rate': random.uniform(5, 15),
                'scraped_at': datetime.now().isoformat()
            }
            trade_records.append(record)
        
        return trade_records
    
    def scan_bds_data(self):
        """Scan BDS compliance and boycott data"""
        print("\n‚úä Scanning BDS compliance data...")
        
        bds_data = []
        
        # BDS-related sources
        bds_sources = [
            'BDS National Committee',
            'WhoProfits Research Center',
            'UN Database of Settlement Goods'
        ]
        
        for source in bds_sources:
            print(f"  ‚öñÔ∏è Scanning: {source}")
            
            try:
                # Create mock BDS data
                mock_bds = self._create_mock_bds_data(source)
                bds_data.extend(mock_bds)
                
                print(f"    üìù Created {len(mock_bds)} BDS records")
                
            except Exception as e:
                print(f"    ‚ùå Error: {e}")
        
        if bds_data:
            self._save_as_csv(bds_data, "bds_data.csv")
            self._save_as_json(bds_data, "bds_data.json")
        
        return bds_data
    
    def _create_mock_bds_data(self, source):
        """Create mock BDS compliance data"""
        bds_records = []
        
        companies = [
            ('Canaan Fair Trade', 'Compliant', 'Food', 'All products settlement-free'),
            ('Hebron Glass Factory', 'Compliant', 'Crafts', '100% Palestinian owned'),
            ('Nablus Soap Co.', 'Compliant', 'Cosmetics', 'Traditional Palestinian production'),
            ('Mixed Source Co.', 'Needs Review', 'Various', 'Some components from settlements'),
            ('International Brand', 'Non-Compliant', 'Retail', 'Operates in settlements')
        ]
        
        for i, (company, status, category, notes) in enumerate(companies):
            record = {
                'id': f"bds_{source.replace(' ', '_').lower()}_{i}",
                'company': company,
                'bds_status': status,
                'category': category,
                'notes': notes,
                'source': source,
                'last_updated': datetime.now().strftime('%Y-%m-%d'),
                'scraped_at': datetime.now().isoformat()
            }
            bds_records.append(record)
        
        return bds_records
    
    def _save_raw_html(self, html, filename):
        """Save raw HTML for processing"""
        filepath = RAW_DATA_DIR / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(html)
    
    def _save_as_csv(self, data, filename):
        """Save data as CSV"""
        filepath = RAW_DATA_DIR / filename
        
        if data:
            # Convert list of dicts to DataFrame
            df = pd.DataFrame(data)
            df.to_csv(filepath, index=False, encoding='utf-8-sig')
            print(f"    üíæ Saved {len(data)} records to {filename}")
    
    def _save_as_json(self, data, filename):
        """Save data as JSON"""
        filepath = RAW_DATA_DIR / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def generate_reports(self):
        """Generate summary reports of collected data"""
        print("\nüìä Generating data reports...")
        
        reports = {
            'scan_summary': {
                'total_scanned': 0,
                'data_sources': [],
                'file_sizes': {},
                'generated_at': datetime.now().isoformat()
            }
        }
        
        # List all generated files
        csv_files = list(RAW_DATA_DIR.glob("*.csv"))
        json_files = list(RAW_DATA_DIR.glob("*.json"))
        
        total_records = 0
        
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file)
                records = len(df)
                total_records += records
                
                reports['scan_summary']['data_sources'].append({
                    'file': csv_file.name,
                    'records': records,
                    'columns': list(df.columns),
                    'size_kb': os.path.getsize(csv_file) // 1024
                })
                
                print(f"  üìÅ {csv_file.name}: {records} records")
                
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Error reading {csv_file.name}: {e}")
        
        reports['scan_summary']['total_scanned'] = total_records
        
        # Save report
        report_file = RAW_DATA_DIR / "scan_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(reports, f, ensure_ascii=False, indent=2)
        
        print(f"\n‚úÖ Scan complete!")
        print(f"üìä Total records collected: {total_records}")
        print(f"üìÅ Raw data saved to: {RAW_DATA_DIR}")
        print(f"üìÑ Report saved to: {report_file}")
        
        return reports
    
    def run_full_scan(self):
        """Run complete scan of all data sources"""
        print("="*70)
        print("üöÄ PALESTINE MARKET DATA SCANNER")
        print("="*70)
        print("Collecting data from multiple sources...")
        
        # Create timestamp for this scan
        scan_id = datetime.now().strftime('%Y%m%d_%H%M')
        print(f"Scan ID: {scan_id}")
        
        # Run all scanners
        businesses = self.scan_websites()
        products = self.scan_marketplaces()
        social = self.scan_social_media()
        trade = self.scan_export_data()
        bds = self.scan_bds_data()
        
        # Generate reports
        reports = self.generate_reports()
        
        # Create scan summary
        summary = {
            'scan_id': scan_id,
            'scan_time': datetime.now().isoformat(),
            'data_collected': {
                'businesses': len(businesses),
                'products': len(products),
                'social_mentions': len(social),
                'trade_records': len(trade),
                'bds_records': len(bds),
                'total': len(businesses) + len(products) + len(social) + len(trade) + len(bds)
            },
            'raw_files': [f.name for f in RAW_DATA_DIR.iterdir() if f.is_file()],
            'next_steps': [
                "1. Process raw data using generate-ai-edible-data.py",
                "2. Import processed data into Palestine Market Scanner",
                "3. Update dashboard with new data"
            ]
        }
        
        # Save summary
        summary_file = RAW_DATA_DIR / "scan_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print("\n" + "="*70)
        print("üéâ SCAN COMPLETE!")
        print("="*70)
        
        print(f"\nüìä SCAN RESULTS:")
        print(f"   Businesses: {len(businesses)}")
        print(f"   Products: {len(products)}")
        print(f"   Social mentions: {len(social)}")
        print(f"   Trade records: {len(trade)}")
        print(f"   BDS records: {len(bds)}")
        print(f"   TOTAL: {summary['data_collected']['total']} records")
        
        print(f"\nüíæ DATA FILES:")
        for file in RAW_DATA_DIR.iterdir():
            if file.is_file():
                size_kb = file.stat().st_size // 1024
                print(f"   {file.name} ({size_kb} KB)")
        
        print(f"\nüéØ NEXT STEPS:")
        for step in summary['next_steps']:
            print(f"   {step}")
        
        print(f"\nüí∞ COST: $0.00")
        print("="*70)
        
        return summary

def main():
    """Main function"""
    scanner = PalestineMarketScanner()
    
    # Run full scan
    scanner.run_full_scan()

if __name__ == "__main__":
    # Create required directories
    os.makedirs("data/raw", exist_ok=True)
    os.makedirs("data/processed", exist_ok=True)
    
    main()